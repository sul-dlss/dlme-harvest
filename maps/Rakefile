require 'csv'
require 'fileutils'
require 'json'
require 'open-uri'

require 'colorize'
require 'mechanize'

task default: %w(all)

task all: %w(import:maps:all)

def setup
  @agent = Mechanize.new
  FileUtils.mkdir_p('./records/harvard')
  FileUtils.mkdir_p('./records/stanford')
  @pids = []
  @include_prefix = %w(harvard stanford)
end

def cleanup
  FileUtils.rm_rf('./records')
end

def collectPid(pid)
  prefix = pid.split('-')[0]
  @pids << pid if @include_prefix.include? prefix
end

def crawl_url(url)
  @agent.get(url) do |doc|
    data = doc.css('//div.documentHeader')
    data.each do |pid|
      collectPid pid.attr('data-layer-id')
    end
  end
end


def cache_remote(link)
  f = File.basename(URI.parse(link).path)
  FileUtils.cd('/Users/waynegraham/projects/dlme-metadata/maps/records/stanford', :verbose => true)

  unless File.file? f
    puts "Downloading file #{link} to ".yellow
    @agent.get(link).save
  else
    puts "File exits (#{link})".green
  end
end

namespace :import do
  namespace :maps do
    setup

    desc 'Import all map objects'
    task all: %w(import:maps:harvard import:maps:stanford)

    desc 'Import Harvard map objects'
    task :harvard do
      harvard_layers   = JSON.parse(File.read('../work/edu.harvard/layers.json'))
      search_url = 'https://earthworks.stanford.edu/catalog?f%5Bdct_provenance_s%5D%5B%5D=Harvard&f%5Blayer_geom_type_s%5D%5B%5D=Raster&per_page=100&bbox=46.400757%2042.269179%2048.729858%2044.190082'
      puts 'Crawling Harvard URLs'.green
      crawl_url search_url

      @pids.each do |pid|
        puts "Locating #{pid}".green

        prefix = pid.split('-')[0]
        suffix = pid.sub("#{prefix}-",'')
        suffix.gsub!(/-/, '_') unless suffix.nil?
        suffix.upcase!
        suffix.prepend('HARVARD.SDE2.')
        path = "../work/edu.harvard/#{harvard_layers[suffix]}/fgdc.xml"
        FileUtils.cp(path, "./records/harvard/#{suffix}.fgdc.xml") if harvard_layers[suffix]
      end
    end

    desc 'Import Stanford Maps'
    task :stanford do
      search_urls = %w(
        https://earthworks.stanford.edu/?bbox=46.400757+42.269179+48.729858+44.190082&f%5Bdct_provenance_s%5D%5B%5D=Stanford&f%5Blayer_geom_type_s%5D%5B%5D=Raster&per_page=100
        https://earthworks.stanford.edu/catalog?f%5Bdct_provenance_s%5D%5B%5D=Stanford&f%5Blayer_geom_type_s%5D%5B%5D=Raster&per_page=100&bbox=17.314453%2021.125498%2054.580078%2053.644638
      )

      ignore_pids = CSV.read('stanford_ignore.csv')
      search_urls.each do |url|
        puts "Crawling Stanford URLs #{url}".green
        crawl_url url

        @pids.each do |pid|
          prefix = pid.split('-')[0]
          suffix = pid.sub("#{prefix}-",'')
          suffix.prepend('https://purl.stanford.edu/')
          suffix += '.mods'

          cache_remote suffix
        end

      end
    end

    task :cleanup_stanford do
      ignore_pids = CSV.read('./stanford_ignore.csv')

      ignore_pids.each do |pid|
        filepath = "records/stanford/#{pid.first}.mods"
        command = "git rm #{filepath}"
        puts `#{command}`
      end
    end
  end
end
